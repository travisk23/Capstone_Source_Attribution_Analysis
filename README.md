# Capstone_Source_Attribution_Analysis
Capstone project for Indiana University that provides correlation analysis of source attribution errors. 2021

The data for this study was obtained from the following site https://osf.io/tnymk/

The researchers of the study sought to determine how the brain encodes memories that origninate from different sources/modalities i.e., a cow seen in real life versus seen in virtual reality. The PDF that is included in this project gives an indepth look at the analysis I performed. The code for the analysis itself can be found in the accomanying python(.py) file included in this repository.

# Conclusions Drawn from The Analyses
Analyses revealed a negative correlation between the error rate and confidence data. We see that as confidence increases error rate decreases. This implies that particpants that are more confident in their choice of source attribution tend to be more accurate in their source recall capablilites. We can determine through this analysis that particpants were most accurate when recalling a source as being a physical object. We also note that particants are nearly as confident when determing computor monitor sources. An intriguing bit of data shows that particpants displayed a wider, as well as lower,  average range of confidence ratings when determining a stimulus source that was experienced in a virtual enviornment. This supports the claims of efficacy that virtual reality has the potetnial to become indistinguishable, in a particpant's memory recall, from reality. To be determined in future studies. 

# What I have learned

During this process I have learned of the wider applications of virtual reality technology. More importantly, this has been my first attempt at processing data on my own. From this project, I was able to learn about Pandas dataframes and their usefulness in data wrangling.  I learned about the uses for differnt libraries like numpy and seaborn, as well as how to extract data from different files and save the combined data into a new file. The data I worked with had to be called from .csv files and required a fair amount of cleaning before it could be properly analyzed. Particularly troublesome was the data in the confidence rating category. This data was filed with "," rather than deceimal points during data stroage. I created a "for" loop to cycle through each row and swap the comma for a deceimal point. This allowed my confidence float variables to be read into the new data frame. I was then able to interact, alter, and project data in ways that were not only novel but useful to me. Using correlational data, scatter plots, and box and whisker plots was an exciting foray into the capabilities of data analyses. This also made me more confident and comfortable in my abilities to handle data analyses in future projects. Looking back this project was straightforward with clear direction, I can only see that now because I have learned what it takes to get to this point.
